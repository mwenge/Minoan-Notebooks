{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Ordering of Names\n",
    "\n",
    "Look at the lexical ordering entities assigned numbers to see if there's any pattern in the ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a list of all inscriptions where entities have quantities assigned to them. \n",
    "We exclude things like commodities, so this should be just a list of words that have both\n",
    "the 'word' tag and the 'assigned number' tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ARKH2': [['*131B',\n",
       "   'A-SI-DA-TO-I',\n",
       "   '*79-*301-SE-DE-*21F-*118',\n",
       "   'A-SU-PU-WA']],\n",
       " 'HT1': [['*79-SU', 'DI-DI-ZA-KE', 'KU-PA₃-NU', 'A-RA-NA-RE']],\n",
       " 'HT102': [['VIR+[?]-GRA+PA', 'DI-RI-NA', 'MA-*79']],\n",
       " 'HT103': [['DA-KU-NA', 'DA-KU-SE-NE']],\n",
       " 'HT104': [['DA-KU-SE-NE-TI', 'I-DU-TI', 'PA-DA-SU-TI']],\n",
       " 'HT108': [['DI-NA-RO', 'RA₂-TI']],\n",
       " 'HT10a': [['DA-RE', 'U-*325-ZA'], ['*305-RU', 'DA-RI-DA', 'ME-ZA']],\n",
       " 'HT10b': [['U-TI', 'DA-RE', 'TA-RI-NA', '*312-TA', 'KA-SA-RU', 'TA-NA-TI']],\n",
       " 'HT115a': [['NA-*21F-NE-MI-NA',\n",
       "   'SE-KU-TU',\n",
       "   'PA-RA-NE',\n",
       "   'A-SE-JA',\n",
       "   'KA-PO-RU']],\n",
       " 'HT117a': [['U-SU',\n",
       "   'MI-TU',\n",
       "   'KU-RA-MU',\n",
       "   'MA-RU',\n",
       "   'KU-PA₃-NU',\n",
       "   'TU-JU-MA',\n",
       "   'U-DI-MI',\n",
       "   'MI-RU-TA-RA-RE',\n",
       "   'TE-JA-RE',\n",
       "   'NA-DA-RE'],\n",
       "  ['KU-KU-DA-RA', 'KO-SA-I-TI', 'DA-MI-NU', 'DA-NE-KU-TI', 'KI-DA-RO']],\n",
       " 'HT117b': [['KU-RE-JU', 'DI-KI-SE']],\n",
       " 'HT119': [['RI-MI-SI', 'KO-JA', 'KU-PA₃-NA-TU'],\n",
       "  ['JA-*345', '*306-TU', '*327-JU']],\n",
       " 'HT11a': [['KA-RO-NA', '*322-RI', 'A-SU-JA', 'VIR+[?]-I']],\n",
       " 'HT122a': [['DA-SI-*118',\n",
       "   'TE-KI',\n",
       "   'QA-*310-I',\n",
       "   'JA-MI-DA-RE',\n",
       "   'SI-DA-RE',\n",
       "   '*324-DI-RA',\n",
       "   'PA-DE',\n",
       "   'KU-PA₃-NU',\n",
       "   'PA-TA-NE',\n",
       "   '*306-TU'],\n",
       "  ['KU-PA₃-NU', 'DA-RI-DA', 'KU-DA']],\n",
       " 'HT122b': [['A-RA-JU-U-DE-ZA', 'QA-QA-RU', 'DA-RE']],\n",
       " 'HT123+124b': [['SI-DU', 'DU-MA-I-NA']],\n",
       " 'HT13': [['TE-TU', 'TE-KI', 'KU-*79-NI', 'DA-SI-*118', 'I-DU-NE-SI']],\n",
       " 'HT15': [['DU-AROM-A', '*586']],\n",
       " 'HT17': [['SA-RO', 'SI-DA-RE']],\n",
       " 'HT19': [['SA-RO', 'DU-ME-DI']],\n",
       " 'HT20': [['KU-MA-JU', '*21M', 'SA-RE-JU']],\n",
       " 'HT23a': [['*21F-RI-TU-QA', 'SA-SA-ME']],\n",
       " 'HT24b': [['SI+ME-KI-*118', '*539-*118']],\n",
       " 'HT25a': [['RU-NI', 'U-RE-WI', 'DI-NA-U', 'A-RI-NI-TA']],\n",
       " 'HT26a': [['TA-TI', 'MI-KI-SE-NA']],\n",
       " 'HT26b': [['RO-NI', 'KA-U-*79-NI']],\n",
       " 'HT29': [['DI-JA-I', 'SA-*323-MI', 'KI-TA', 'A-RE-DA-I', 'KA-DU-MA-NE']],\n",
       " 'HT30': [['SA-RA-RA', '*23M']],\n",
       " 'HT31': [['*815-QA-PA₃', '*415-VS-SU-PU', '*416-VS-KA-RO-PA₃'],\n",
       "  ['*815', '*815-SU-PA₃-RA', '*815-PA-TA-QE']],\n",
       " 'HT36': [['*307+*387-GRA+QE', 'DU-*79-WA']],\n",
       " 'HT49a': [['SI-RA', 'A-RU'], ['KU-PA₃-NU', 'TU-SU-PU₂']],\n",
       " 'HT6a': [['MA-*321', 'O-RA₂-DI-NE', 'KA-PA-QE']],\n",
       " 'HT6b': [['WA-DU-NI-MI',\n",
       "   'RA-TI-SE',\n",
       "   'MA-RI-RE-I',\n",
       "   'DU-DA-MA',\n",
       "   'DA-KI',\n",
       "   'SA-MA',\n",
       "   'PA₃-NI-NA']],\n",
       " 'HT7a': [['I-RU-JA', 'DU-JA', 'TA-NA-TI', 'DA-RE', 'TE-TU']],\n",
       " 'HT7b': [['*21F-TU-NE', 'DA-RU-*329']],\n",
       " 'HT85a': [['DA-RI-DA',\n",
       "   'PA₃-NI',\n",
       "   'U-*325-ZA',\n",
       "   'DA-SI-*118',\n",
       "   'KU-*79-NI',\n",
       "   'TE-KE',\n",
       "   'DA-RE']],\n",
       " 'HT85b': [['KI-RE-TA₂',\n",
       "   'QE-KA',\n",
       "   'ME-ZA',\n",
       "   'RE-DI-SE',\n",
       "   'WA-DU-NI-MI',\n",
       "   'MA-DI',\n",
       "   'QA-*310-I']],\n",
       " 'HT86a': [['*584', 'SA-RU', 'DI-DE-RU', 'QA-RA₂-WA']],\n",
       " 'HT86b': [['*584', 'SA-RU']],\n",
       " 'HT87': [['PI-TA-KE-SI', 'JA-RE-MI', 'DI-KI-SE', 'QE-SU-PU', 'KU-RU-KU']],\n",
       " 'HT88': [['KU-PA₃-PA₃',\n",
       "   'KA-JU',\n",
       "   'KU-PA₃-NU',\n",
       "   'PA-JA-RE',\n",
       "   'SA-MA-RO',\n",
       "   'DA-TA-RE']],\n",
       " 'HT8a': [['PA₃-KA-RA-TI', 'TE-*301', 'QA-*310-I', 'SI-KI-RA', 'KI-RE-TA-NA']],\n",
       " 'HT8b': [['PA₃-*188', 'QA-*310-I', 'PA-JA-RE']],\n",
       " 'HT93a': [['KI-DI-NI', 'SA-RA₂'], ['VIR+[?]-I', 'DE-JU-KU', '*406-VS']],\n",
       " 'HT94b': [['TU-MA', 'PA-TA-NE', 'DE-DI', 'KE-KI-RU', 'SA-RU']],\n",
       " 'HT95a': [['DA-ME', 'MI-NU-TE', 'SA-RU', 'KU-NI-SU', 'DI-DE-RU', 'QE-RA₂-U']],\n",
       " 'HT95b': [['DA-ME', 'MI-NU-TE', 'KU-NI-SU', 'DI-DE-RU', 'QE-RA₂-U']],\n",
       " 'HT96a': [['RU-SA', '*323-*317']],\n",
       " 'HT97a': [['KA-NU-TI', 'PA-I-TO', 'NA-TI', 'MA-DI']],\n",
       " 'HT98a': [['TA-NA-TI', 'DI-RE-DI-NA', 'TE-*301', 'RO-KE', 'KA-RI-*310-I']],\n",
       " 'HT99b': [['SI+SE-NI', 'DA-SI-*118', '*79-DU', 'RU-MA-TA']],\n",
       " 'HT9a': [['*306-TU', 'DI-NA-U', 'QE-PU', '*324-DI-RA', 'TA-I-AROM', 'A-RU']],\n",
       " 'HT9b': [['PA-DE',\n",
       "   'A-SI',\n",
       "   '*306-TU',\n",
       "   '*324-DI-RA',\n",
       "   'QE-PU',\n",
       "   'TA-I-AROM',\n",
       "   'DI-NA-U']],\n",
       " 'MA2c': [['U-NA-NA', 'JA-MA-U-TI']],\n",
       " 'PE2': [['RU-PI-*305-MI',\n",
       "   'A-*325-ZA',\n",
       "   'A-RI-PA',\n",
       "   'QA-QA-DA',\n",
       "   'TO-ME',\n",
       "   'TO-*49-RE']],\n",
       " 'PH(?)31a': [['*21M', 'AU-SI-RE']],\n",
       " 'PH(?)31b': [['*22F', '*22M'], ['NE-*22M', '*22F']],\n",
       " 'PH2': [['A-SE-TU-*21F', 'RA-O-DI-KI', 'PI-RU-E-JU', 'SE-SA-PA₃']],\n",
       " 'PH3a': [['*557', '*563']],\n",
       " 'PK1': [['NE-TI', 'KA-QA'],\n",
       "  ['A-DU-ZA', 'TA₂-TA-RE'],\n",
       "  ['TA₂-TI-TE', 'O-KA-MI-ZA-SI-I-NA'],\n",
       "  ['RA-NA-TU-SU', 'NI-MI'],\n",
       "  ['TU-SU', 'MA-TI-ZA-I-TE'],\n",
       "  ['MA-TE-TI', 'MA-KA-I-TA']],\n",
       " 'THEZb13': [['QI-VIN+TE', 'KA-A-SI-TE']],\n",
       " 'TY2': [['*309B-KI-PU', '*309-JU-KI'],\n",
       "  ['*309C-RU-KA', '*309-RI-WA-JU', '*309B-RI-JU']],\n",
       " 'ZA10a': [['A-KU-MI-NA',\n",
       "   'A-TA-NA-TE',\n",
       "   'A-MI-DA-U',\n",
       "   'A-DU-KU-MI-NA',\n",
       "   'DA-I-PI-TA',\n",
       "   'DU-RE-ZA-SE']],\n",
       " 'ZA10b': [['WA-*362',\n",
       "   '*131B',\n",
       "   'U-*49',\n",
       "   'MA-ZA',\n",
       "   'MA-KI-DE-TE',\n",
       "   'SA-MA',\n",
       "   'A-DE',\n",
       "   'A-MI-TA',\n",
       "   'RA₂-RO-RE',\n",
       "   'PA-JA-RE',\n",
       "   'KA-KU-NE-TE']],\n",
       " 'ZA14': [['ME-KI-DI', '*21F-*118', 'PU-NI-KA-*363', 'QA-TI-JU', 'KU-PI']],\n",
       " 'ZA15a': [['QE-SI-*79-E', 'I-TI-NI-SA'],\n",
       "  ['MI-ZA-SE',\n",
       "   '*28B-NU-MA-RE',\n",
       "   'SI-PI-KI',\n",
       "   'JA-SA-MU',\n",
       "   'SA-MI-DA-E',\n",
       "   '*363-KE-MA-SE']],\n",
       " 'ZA20': [['SI-TE-TU', 'SI-TU', 'TE-AROM', 'RU-MA-TA-SE']],\n",
       " 'ZA4a': [['A-TI-RU',\n",
       "   'TU-ME-SE',\n",
       "   'QE-SI-*79-E',\n",
       "   '*28B-NU-MA-RE',\n",
       "   'SI-PI-KI',\n",
       "   'KA-DI']],\n",
       " 'ZA5a': [['KI-NI-MA', 'O-TA-NI-ZA-SE']],\n",
       " 'ZA5b': [['*28B-NU-MA-RE', 'SI-PI-KI', 'MA-KA-I-TA']],\n",
       " 'ZA7a': [['U-JU', 'A-RA-TU']],\n",
       " 'ZA8': [['KU-TU-KO-RE', 'TA-I-NU-MA-PA', 'MA-KA-I-SE', 'DA-I-PI-TA']],\n",
       " 'ZA9': [['*22F', '*22F']]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "files = [\"060-transaction-words.txt\", \"160-transaction-signs.txt\"]\n",
    "words_to_ignore = []\n",
    "for file in files:\n",
    "    input_file = open(\"../\" + file, 'r')\n",
    "    while True:\n",
    "        line = input_file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        words_to_ignore.extend(line.strip().split('\\t'))\n",
    "\n",
    "json_file = open('../210-wordtags.js')\n",
    "inscriptions = json.load(json_file)\n",
    "\n",
    "words_before = {}\n",
    "words_after = {}\n",
    "\n",
    "def atStartOfList(word_tags, index):\n",
    "    if index < 2:\n",
    "        return False\n",
    "    word_tag = word_tags[index - 3]\n",
    "    if \"word\" in word_tag[\"tags\"] and \"assigned number\" in word_tag[\"tags\"]:\n",
    "        return False\n",
    "    if \"number\" in word_tag[\"tags\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "assignment_sequences = {}\n",
    "for inscription in inscriptions:\n",
    "    word_tags = inscription[\"tagsForWords\"]\n",
    "\n",
    "    sequences = []\n",
    "    sequence = []\n",
    "    for index, word_tag in enumerate(word_tags):\n",
    "        tags = word_tag[\"tags\"]\n",
    "        if \"word\" not in tags or \"assigned number\" not in tags:\n",
    "            continue\n",
    "        word = word_tag[\"transliteratedWord\"]\n",
    "        if word in words_to_ignore:\n",
    "            continue\n",
    "        if atStartOfList(word_tags, index):\n",
    "            if sequence and len(sequence) > 1:\n",
    "                sequences.append(sequence)\n",
    "            sequence = []\n",
    "        sequence.append(word)\n",
    "    if sequence and len(sequence) > 1:\n",
    "        sequences.append(sequence)\n",
    "    if sequences:\n",
    "        assignment_sequences[inscription[\"name\"]] = sequences\n",
    "\n",
    "assignment_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the instances where a letter sees other ordered before and after it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try to see if there's any evidence of the assignees appearing in a consistent lexical order. We do this by seeing the initial letters of each entry show always appear before or after each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "--------------\n",
      "letters in common\n",
      "--------------\n",
      "*79 ['A']\n",
      "*79 prev_words {'A': 3}\n",
      "*79 after_words {'A': 2}\n",
      "DI ['A', 'KU', 'QE']\n",
      "DI prev_words {'A': 2, 'KU': 5, 'QE': 3}\n",
      "DI after_words {'A': 2, 'KU': 2, 'QE': 4}\n",
      "KU ['KU', 'QE', 'MI', 'DI', 'TE', 'U', 'PA', 'SA', 'DA']\n",
      "KU prev_words {'KU': 17, 'QE': 2, 'MI': 5, 'DI': 4, 'TE': 2, 'U': 4, 'PA': 2, 'SA': 3, 'DA': 10}\n",
      "KU after_words {'KU': 3, 'QE': 2, 'MI': 2, 'DI': 3, 'TE': 3, 'U': 2, 'PA': 3, 'SA': 3, 'DA': 7}\n",
      "A ['KA', 'TO', 'A', 'RA₂', 'QA', 'DU', 'PA', 'DA']\n",
      "A prev_words {'KA': 5, 'TO': 4, 'A': 39, 'RA₂': 2, 'QA': 3, 'DU': 4, 'PA': 4, 'DA': 5}\n",
      "A after_words {'KA': 4, 'TO': 4, 'A': 9, 'RA₂': 2, 'QA': 3, 'DU': 4, 'PA': 2, 'DA': 5}\n",
      "MA ['KA', 'A', 'RA₂', 'PA', 'SA', 'MA']\n",
      "MA prev_words {'KA': 4, 'A': 4, 'RA₂': 2, 'PA': 3, 'SA': 2, 'MA': 15}\n",
      "MA after_words {'KA': 3, 'A': 4, 'RA₂': 2, 'PA': 2, 'SA': 2, 'MA': 2}\n",
      "O not enough info [{'O': 3}]\n",
      "KA ['PA']\n",
      "KA prev_words {'PA': 4}\n",
      "KA after_words {'PA': 2}\n",
      "RA not enough info [{'RA': 3}]\n",
      "DU ['DA']\n",
      "DU prev_words {'DA': 3}\n",
      "DU after_words {'DA': 2}\n",
      "DA ['TE', 'KU', 'DA', 'KI']\n",
      "DA prev_words {'TE': 2, 'KU': 8, 'DA': 19, 'KI': 2}\n",
      "DA after_words {'TE': 4, 'KU': 6, 'DA': 5, 'KI': 2}\n",
      "SA ['DI']\n",
      "SA prev_words {'DI': 2}\n",
      "SA after_words {'DI': 2}\n",
      "PA₃ ['DA']\n",
      "PA₃ prev_words {'DA': 3}\n",
      "PA₃ after_words {'DA': 2}\n",
      "I not enough info [{'I': 3, 'DA': 2}]\n",
      "TA []\n",
      "TA prev_words {}\n",
      "TA after_words {}\n",
      "TE ['SI']\n",
      "TE prev_words {'SI': 3}\n",
      "TE after_words {'SI': 2}\n",
      "*21F not enough info [{'*21F': 3}]\n",
      "QA ['TO']\n",
      "QA prev_words {'TO': 2}\n",
      "QA after_words {'TO': 2}\n",
      "SI ['TE', 'RU']\n",
      "SI prev_words {'TE': 4, 'RU': 2}\n",
      "SI after_words {'TE': 2, 'RU': 2}\n",
      "KI not enough info [{'QA': 2, 'KI': 5, 'DA': 2}]\n",
      "PA ['A', 'KA', 'SA']\n",
      "PA prev_words {'A': 4, 'KA': 3, 'SA': 2}\n",
      "PA after_words {'A': 2, 'KA': 2, 'SA': 2}\n",
      "*306 ['*324']\n",
      "*306 prev_words {'*324': 2}\n",
      "*306 after_words {'*324': 2}\n",
      "QE not enough info [{'*306': 2, 'DI': 4, 'QE': 5, '*324': 2, 'KU': 3, 'DA': 2, 'MI': 2}]\n",
      "*324 []\n",
      "*324 prev_words {}\n",
      "*324 after_words {}\n",
      "ME []\n",
      "ME prev_words {}\n",
      "ME after_words {}\n",
      "U ['KU', 'MI', 'A', 'TE', 'MA', 'DA', 'NA']\n",
      "U prev_words {'KU': 5, 'MI': 4, 'A': 4, 'TE': 3, 'MA': 4, 'DA': 4, 'NA': 2}\n",
      "U after_words {'KU': 3, 'MI': 3, 'A': 4, 'TE': 3, 'MA': 3, 'DA': 3, 'NA': 2}\n",
      "VIR+[?] not enough info [{'VIR+[?]': 2}]\n",
      "RU not enough info [{'RU': 4, 'SI': 2}]\n",
      "MI ['TE', 'NA', 'KU']\n",
      "MI prev_words {'TE': 2, 'NA': 2, 'KU': 5}\n",
      "MI after_words {'TE': 2, 'NA': 2, 'KU': 4}\n",
      "RO ['KA']\n",
      "RO prev_words {'KA': 2}\n",
      "RO after_words {'KA': 2}\n",
      "*815 ['*815']\n",
      "*815 prev_words {'*815': 10}\n",
      "*815 after_words {'*815': 3}\n",
      "TU not enough info [{'KU': 3, 'TU': 5, 'U': 2, 'MI': 2, 'MA': 2}]\n",
      "PI not enough info [{'PI': 2}]\n",
      "JA ['*306', 'KU', 'PA']\n",
      "JA prev_words {'*306': 2, 'KU': 2, 'PA': 2}\n",
      "JA after_words {'*306': 2, 'KU': 2, 'PA': 2}\n",
      "DE not enough info [{'DE': 2}]\n",
      "NA not enough info [{'KA': 2, 'PA': 2, 'NA': 3, 'MA': 2, 'U': 2, 'MI': 2, 'KU': 2}]\n",
      "SE not enough info [{'SE': 2, 'A': 2}]\n",
      "KO ['DA']\n",
      "KO prev_words {'DA': 2}\n",
      "KO after_words {'DA': 2}\n",
      "TO not enough info [{'A': 4, 'QA': 2, 'TO': 4}]\n",
      "NE not enough info [{'NE': 2}]\n",
      "TA₂ not enough info [{'TA₂': 2}]\n",
      "*131B ['A', 'MA']\n",
      "*131B prev_words {'A': 2, 'MA': 2}\n",
      "*131B after_words {'A': 4, 'MA': 2}\n",
      "RA₂ not enough info [{'MA': 2, 'A': 2}]\n",
      "*584 not enough info [{'SA': 2}]\n",
      "*28B not enough info [{'SI': 3}]\n",
      "\n",
      "\n",
      "letters not in common\n",
      "--------------\n",
      "*79\n",
      "*79 prev_words {'*79': 3}\n",
      "*79 after_words {}\n",
      "DI\n",
      "DI prev_words {'DI': 7, '*306': 2, '*324': 2, 'TA': 2, 'SA': 3, 'DA': 2, 'MI': 2}\n",
      "DI after_words {}\n",
      "KU\n",
      "KU prev_words {'KA': 2, 'MA': 2, 'QA': 2, 'JA': 2, 'KO': 2}\n",
      "KU after_words {'TU': 3, 'NA': 2}\n",
      "A\n",
      "A prev_words {'*79': 3, 'DI': 3, 'U': 4, 'SE': 2, '*131B': 4, 'MA': 4, 'SA': 2}\n",
      "A after_words {}\n",
      "MA\n",
      "MA prev_words {'NA': 2, 'U': 4, 'MI': 2, 'KU': 2, 'TU': 2, '*131B': 2}\n",
      "MA after_words {}\n",
      "O not enough info [{'O': 3}]\n",
      "KA\n",
      "KA prev_words {'MA': 4, 'KA': 12, 'TA': 2, 'A': 5, 'RO': 2, 'KU': 2, 'SA': 2, 'NA': 2}\n",
      "KA after_words {}\n",
      "RA not enough info [{'RA': 3}]\n",
      "DU\n",
      "DU prev_words {'DU': 5, 'A': 4}\n",
      "DU after_words {}\n",
      "DA\n",
      "DA prev_words {'DU': 3, 'I': 2, 'U': 3, 'PA₃': 2, 'A': 5, 'SA': 2, 'KO': 2}\n",
      "DA after_words {'PA': 2, 'MI': 2, 'DI': 2, 'QE': 2}\n",
      "SA\n",
      "SA prev_words {'SA': 9, 'MA': 2, 'KU': 3, '*584': 2, 'PA': 2, 'DA': 2}\n",
      "SA after_words {'A': 2}\n",
      "PA₃\n",
      "PA₃ prev_words {'PA₃': 3}\n",
      "PA₃ after_words {}\n",
      "I not enough info [{'I': 3, 'DA': 2}]\n",
      "TA\n",
      "TA prev_words {'TA': 5, 'DA': 2}\n",
      "TA after_words {'TE': 2, 'DI': 2}\n",
      "TE\n",
      "TE prev_words {'TA': 2, 'DA': 5, 'TE': 7, 'U': 3, 'KU': 3, 'MI': 2}\n",
      "TE after_words {'QA': 2, 'PA': 2}\n",
      "*21F not enough info [{'*21F': 3}]\n",
      "QA\n",
      "QA prev_words {'TE': 2, 'QA': 5, 'KI': 2, 'ME': 2, 'DA': 2, 'A': 3}\n",
      "QA after_words {'SI': 2, 'PA': 3, 'KU': 2}\n",
      "SI\n",
      "SI prev_words {'QA': 2, 'SI': 9, '*28B': 3}\n",
      "SI after_words {'PA': 2}\n",
      "KI not enough info [{'QA': 2, 'KI': 5, 'DA': 2}]\n",
      "PA\n",
      "PA prev_words {'QA': 3, 'PA': 6, 'KU': 3, 'MA': 3, '*324': 3, 'DA': 2, 'TE': 2, 'JA': 2, 'SI': 2, 'NA': 2}\n",
      "PA after_words {'*306': 3}\n",
      "*306\n",
      "*306 prev_words {'PA': 3, 'JA': 2}\n",
      "*306 after_words {'DI': 2, 'QE': 2}\n",
      "QE not enough info [{'*306': 2, 'DI': 4, 'QE': 5, '*324': 2, 'KU': 3, 'DA': 2, 'MI': 2}]\n",
      "*324\n",
      "*324 prev_words {'*306': 2}\n",
      "*324 after_words {'PA': 2}\n",
      "ME\n",
      "ME prev_words {'ME': 2}\n",
      "ME after_words {'QA': 2}\n",
      "U\n",
      "U prev_words {'U': 10, 'TU': 2}\n",
      "U after_words {}\n",
      "VIR+[?] not enough info [{'VIR+[?]': 2}]\n",
      "RU not enough info [{'RU': 4, 'SI': 2}]\n",
      "MI\n",
      "MI prev_words {'MI': 7, 'DA': 2, 'U': 4, 'MA': 2, 'TU': 2}\n",
      "MI after_words {'DI': 2, 'QE': 2}\n",
      "RO\n",
      "RO prev_words {'RO': 2}\n",
      "RO after_words {}\n",
      "*815\n",
      "*815 prev_words {}\n",
      "*815 after_words {}\n",
      "TU not enough info [{'KU': 3, 'TU': 5, 'U': 2, 'MI': 2, 'MA': 2}]\n",
      "PI not enough info [{'PI': 2}]\n",
      "JA\n",
      "JA prev_words {'JA': 5}\n",
      "JA after_words {}\n",
      "DE not enough info [{'DE': 2}]\n",
      "NA not enough info [{'KA': 2, 'PA': 2, 'NA': 3, 'MA': 2, 'U': 2, 'MI': 2, 'KU': 2}]\n",
      "SE not enough info [{'SE': 2, 'A': 2}]\n",
      "KO\n",
      "KO prev_words {'KU': 2, 'KO': 2}\n",
      "KO after_words {}\n",
      "TO not enough info [{'A': 4, 'QA': 2, 'TO': 4}]\n",
      "NE not enough info [{'NE': 2}]\n",
      "TA₂ not enough info [{'TA₂': 2}]\n",
      "*131B\n",
      "*131B prev_words {}\n",
      "*131B after_words {}\n",
      "RA₂ not enough info [{'MA': 2, 'A': 2}]\n",
      "*584 not enough info [{'SA': 2}]\n",
      "*28B not enough info [{'SI': 3}]\n"
     ]
    }
   ],
   "source": [
    "words_before = {}\n",
    "words_after = {}\n",
    "\n",
    "# For every assignee record the assignees that appear before and after it in the list.\n",
    "for i, (inscription, sequences) in enumerate(assignment_sequences.items()):\n",
    "    for sequence in sequences:\n",
    "        if len(sequence) == 1:\n",
    "            continue\n",
    "        prev_words = []\n",
    "        for word in sequence:\n",
    "            if word in words_before:\n",
    "                words_before[word] = words_before[word] + prev_words\n",
    "            else:\n",
    "                words_before[word] = prev_words\n",
    "            for prev_word in prev_words:\n",
    "                if prev_word in words_after:\n",
    "                    words_after[prev_word].append(word)\n",
    "                else:\n",
    "                    words_after[prev_word] = [word]\n",
    "            prev_words.append(word)\n",
    "\n",
    "def reduce(original):\n",
    "    d = {}\n",
    "    for i, (k, v) in enumerate(original.items()):\n",
    "        j = k.split('-')[0]\n",
    "        if j in d:\n",
    "            d[j].extend([x.split('-')[0] for x in v])\n",
    "        else:\n",
    "            d[j] = [x.split('-')[0] for x in v]\n",
    "    return d\n",
    "\n",
    "letter_counts = {}\n",
    "for words in [words_before, words_after]:\n",
    "    print(\"--------------\")\n",
    "    initials = reduce(words)\n",
    "    \n",
    "    for i, (word, prev_words) in enumerate(initials.items()):\n",
    "        A = Counter([x for x in prev_words])\n",
    "        counted = {x : A[x] for x in A if A[x] > 1}\n",
    "        if not counted:\n",
    "            continue\n",
    "        if word in letter_counts:\n",
    "            letter_counts[word].append(counted)\n",
    "        else:\n",
    "            letter_counts[word] = [counted]\n",
    "\n",
    "print(\"letters in common\")\n",
    "print(\"--------------\")\n",
    "for i, (letter, counts) in enumerate(letter_counts.items()):\n",
    "    if len(counts) < 2:\n",
    "        print(letter, \"not enough info\", counts)\n",
    "        continue\n",
    "    common_letters = list(set([x for x in counts[0]]) & set([x for x in counts[1]]))\n",
    "    print(letter, common_letters)\n",
    "    print(letter, \"prev_words\", {x : letter_counts[letter][0][x] for x in common_letters})\n",
    "    print(letter, \"after_words\", {x : letter_counts[letter][1][x] for x in common_letters})\n",
    "\n",
    "print(\"\\n\\nletters not in common\")\n",
    "print(\"--------------\")\n",
    "for i, (letter, counts) in enumerate(letter_counts.items()):\n",
    "    if len(counts) < 2:\n",
    "        print(letter, \"not enough info\", counts)\n",
    "        continue\n",
    "    print(letter)\n",
    "    unique_letters = [x for x in counts[0] if x not in counts[1]]\n",
    "    print(letter, \"prev_words\", {x : letter_counts[letter][0][x] for x in unique_letters})\n",
    "    unique_letters = [x for x in counts[1] if x not in counts[0]]\n",
    "    print(letter, \"after_words\", {x : letter_counts[letter][1][x] for x in unique_letters})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for cases where the same assignees appear together more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
